{"cells":[{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["\n","######## METHOD DEFINITION, CONFIG, and IMPORTS\n","\n","import csv\n","from typing import List, Dict, Tuple\n","import collections\n","from pathlib import Path\n","import pandas as pd\n","import numpy as np\n","import codecs\n","import matplotlib.pyplot as plt\n","\n","import time\n","import sys\n","from tqdm import tqdm\n","\n","from gensim.models import Word2Vec\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","import sys\n","# Include the current directory in notebook path\n","sys.path.insert(0, './')\n","\n","from core.parser import Parser\n","from core.query import DatasetQuery\n","from core.dataset import Dataset\n","from core.preprocessor import Preprocessor\n","from core.data import Data\n","from core.embedding import Embedding\n","from core.birch import BirchNode, ClusteringFeature, BirchTree, BirchDriver\n","from core.postprocessor import PostProcessor\n","\n","# CONFIGS and CONSTANTS\n","DTA_FOLDER_PATH = Path(\"./dataset\")\n","TRAIN_FILE_NAME = Path(\"mlchallenge_set_2021.tsv\")\n","TRAIN_FILE_PATH = DTA_FOLDER_PATH / TRAIN_FILE_NAME\n","VALID_FILE_NAME = Path(\"mlchallenge_set_validation.tsv\")\n","VALID_FILE_PATH = DTA_FOLDER_PATH / VALID_FILE_NAME\n","\n","DATASET_BINARY_FILE = Path(\"dataset.pkl\")\n","DATASET_BINARY_PATH = DTA_FOLDER_PATH / DATASET_BINARY_FILE\n","\n","RESULT_FOLDER_PATH = Path(\"./results\")\n","TREE_FOLDER_PATH = Path(\"./trees\")\n","\n","### CODE BLOCK:\n","# Read file\n","def test_read_file():\n","    s = []\n","    with codecs.open(TRAIN_FILE_PATH, 'r', encoding=\"unicode_escape\") as f:\n","        r = f.readline()\n","        s.append(r)\n","        cnt = 1\n","        total = 1\n","        while r:\n","            total += 1\n","            try:\n","                r = f.readline()\n","                s.append(r)\n","                cnt += 1\n","            except:\n","                continue\n","        print(f\"Read {cnt} lines out of {total} lines.\")\n","\n","# Test read line!\n","def test_read_line(s):\n","    head = 1\n","    left = 10\n","    for i in range (head):\n","        row = s[left + i]\n","        print(row)\n","\n","# get_dataset_query\n","def get_dataset_query():\n","    d = get_dataset()\n","    return DatasetQuery(d)\n","    \n","# Print random data\n","# def print_random_data(q: DatasetQuery):\n","#     q.print_random_data()\n","\n","# Test parsing\n","def test_parse_attributes():\n","    import re\n","    import collections\n","\n","    attr = \"(Colors:blue, white,Special Note::very nice,Style: Modern, sdf, sadf  sd, sdf.,,asd,dad)\"\n","    parse_string = attr[1:-1]\n","    colon_separated = re.split(r\"\\s*:+\\s*\", parse_string)\n","    # Actual return data\n","    attr_data = collections.defaultdict(list)\n","    # Current key is guaranteed to be not None\n","    cur_key = None\n","\n","    for i, element in enumerate(colon_separated):\n","        # Separated by comma\n","        comma_separated = re.split(r\"\\s*,\\s*\", element)\n","        # If it is not the last item, and parse according to the pattern\n","        if i != len(colon_separated) - 1:\n","            for j, element_attr in enumerate(comma_separated):\n","                # If it is the last element\n","                if j == len(comma_separated) - 1:\n","                    cur_key = element_attr.lower()\n","                else:\n","                    attr_data[cur_key].append(element_attr.lower())\n","        # Else, all element in the comma separated \n","        #   list is the attribute of the last key\n","        else:\n","            for j, element_attr in enumerate(comma_separated):\n","                attr_data[cur_key].append(element_attr.lower())\n","\n","    print(str(attr_data))\n","\n","def get_dataset() -> Dataset:\n","    p = Parser(TRAIN_FILE_PATH)\n","    dataset = p.parse_data_file()\n","    return dataset\n","\n","# Get image and write file\n","def test_get_image_and_write_file(uri: str, path: str):\n","    import requests\n","    from PIL import Image\n","    f = open(path,'wb')\n","    f.write(requests.get(uri).content)\n","    f.close()\n","    f = Image.open(path)\n","    # data = np.asarray(f)\n","\n","# Get image and save to variabel for numpy array\n","def get_image_and_save_to_variable(uri: str) -> np.array:\n","    import io\n","    import requests\n","    from PIL import Image\n","    import numpy as np\n","    res = requests.get(uri)\n","    image_bytes = io.BytesIO(res.content)\n","    img = Image.open(image_bytes)\n","    img_mat = np.asarray(img)\n","    return img_mat\n","\n","# Test image work\n","# q = get_dataset_query()\n","# dta = q.get_random_data()\n","# uri = dta.primary_image_url\n","# img_dta = dta.get_image(dta.primary_image_url)\n","# np_img_dta = np.asarray(img_dta)\n","# print(\"Shape: \" + str(np_img_dta.shape))\n","\n","def print_random_data(q: DatasetQuery, category: int):\n","    # Get randome data and image\n","    dta = q.get_random_data(category)\n","    print(dta)\n","    return dta.get_image(dta.primary_image_url)\n","\n","def print_data(d: Dataset, data_id: int) -> None:\n","    dta = d.get_data(data_id)\n","    print(dta)\n","    plt.figure()\n","    plt.imshow(dta.get_image(dta.primary_image_url))\n","    plt.show()\n","\n","def print_list_data(d: Dataset, l: List[int]) -> None:\n","    for data_id in l:\n","        print_data(d, data_id)\n","\n","def print_similarity(model: Word2Vec, d: Dataset, key:str, id1: int, id2: int) -> float:\n","    v1: np.ndarray = Preprocessor.encode_sentence(model, d, id1, key)\n","    v2: np.ndarray = Preprocessor.encode_sentence(model, d, id2, key)\n","    return float(cosine_similarity(v1.reshape(1,-1), v2.reshape(1,-1)))\n","\n","def matplot_plot(dataset: Dataset):\n","    # Print random data in each category\n","    import matplotlib.pyplot as plt\n","    import random\n","    rows = 4\n","    cols = 4\n","    for k, v in dataset.category_map.items():\n","        fig = plt.figure()\n","        axes=[]\n","        print(f\"Category {k}: \")\n","        for a in range(rows*cols):\n","            id = random.randint(0, len(v) - 1)\n","            b = dataset.dataset[v[id]].get_image(dataset.dataset[v[id]].primary_image_url)\n","            axes.append( fig.add_subplot(rows, cols, a+1) )\n","            # subplot_title=(\"Subplot\"+str(a))\n","            # axes[-1].set_title(subplot_title)\n","            plt.imshow(b)\n","        # fig.tight_layout()\n","        plt.show()\n","\n","def get_attr_str(dta: Data):\n","    dta.attributes\n","    t = [k + \" \" + (\" \".join(v)) for k, v in dta.attributes.items()]\n","    t = \" \".join(t)\n","    return t\n","\n","def print_first_frequent_keys(d, category: int, head: int=15):\n","    pre = Preprocessor(d)\n","    w = pre.get_key_weights(category)\n","    odic = sorted(w.items(), key=lambda kv:(kv[1], kv[0]), reverse=True)\n","    value_iter = iter(odic)\n","    for _ in range(head):\n","        print(next(value_iter))\n","\n","def write_all_data(q: DatasetQuery):\n","\n","    # Write data analysis every unique attrs\n","    p = \"./analysis/\"\n","    for i in range (1, 6):\n","        dd = q.get_all_unique_key_attributes(i)\n","        \n","        name = p + \"Unique_attrs_cat\" + str(i) + \".txt\"\n","        DatasetQuery.export_txt(dd, name)\n","\n","    p = \"./analysis/\"\n","    for i in range (1, 6):\n","        dd = q.get_all_unique_key_attributes(i)\n","        name = p + \"unique_attr_cnt\" + str(i) + \".csv\"\n","        # DatasetQuery.export_txt(dd, name)\n","        DatasetQuery.export_csv(dd.items(), name)\n","\n","def write_all_unique_key_value_cnt(q: DatasetQuery):\n","    analysis_path = \"./analysis/\"\n","    #  Export csv of every value\n","    for i in range (1, 6):\n","        cnt = q.get_all_unique_key_value_attrs(i)\n","        # Convert from cnt to 2d list\n","        # cnt: Dict[Counter] \n","        #   or Dict[Dict[int]]: attrs_key, attr_value(key), count\n","        rows = []\n","        for key, attr_obs in cnt.items():\n","            for attr, count in attr_obs.items():\n","                rows.append([key, attr, count])\n","        name = analysis_path + \"unique_attr_key_val_cnt\" + str(i) + \".csv\"\n","        DatasetQuery.export_csv(rows, name)\n","\n","\n","def test_vectorize_all_database(d: Dataset, category: int):\n","    models: Dict[str, Word2Vec] = collections.defaultdict()\n","    sentences_matrix: Dict[str, List[List[str]]]= collections.defaultdict(list)\n","\n","    print(\"Extracting value data sentence to glob...\")\n","    with tqdm(total=len(d.category_map[str(category)])) as pbar:\n","        for data_id in d.category_map[str(category)]:\n","            data = d.dataset[data_id]\n","            for key_attr, val_attrs in data.attributes.items():\n","                sentences_matrix[key_attr].append(val_attrs)\n","            pbar.update(1)\n","    print(\"Done!\")\n","\n","    print(\"Creating Word2Vec models from globs...\")\n","    with tqdm(total=len(sentences_matrix.items())) as pbar:\n","        for key, sentences in sentences_matrix.items():\n","            models[key] = Word2Vec(sentences, min_count=1)\n","            pbar.update(1)\n","    print(\"Done!\")\n","\n","\n",""]},{"cell_type":"code","execution_count":1,"metadata":{},"outputs":[],"source":["\n","######## METHOD DEFINITION, CONFIG, and IMPORTS\n","\n","import csv\n","from typing import List, Dict, Tuple\n","import collections\n","from pathlib import Path\n","import pandas as pd\n","import numpy as np\n","import codecs\n","import matplotlib.pyplot as plt\n","\n","import time\n","import sys\n","from tqdm import tqdm\n","\n","from gensim.models import Word2Vec\n","from sklearn.metrics.pairwise import cosine_similarity\n","\n","import sys\n","# Include the current directory in notebook path\n","sys.path.insert(0, './')\n","\n","from core.parser import Parser\n","from core.query import DatasetQuery\n","from core.dataset import Dataset\n","from core.preprocessor import Preprocessor\n","from core.data import Data\n","from core.embedding import Embedding\n","from core.birch import BirchNode, ClusteringFeature, BirchTree, BirchDriver\n","from core.postprocessor import PostProcessor\n","\n","# CONFIGS and CONSTANTS\n","DTA_FOLDER_PATH = Path(\"./dataset\")\n","TRAIN_FILE_NAME = Path(\"mlchallenge_set_2021.tsv\")\n","TRAIN_FILE_PATH = DTA_FOLDER_PATH / TRAIN_FILE_NAME\n","VALID_FILE_NAME = Path(\"mlchallenge_set_validation.tsv\")\n","VALID_FILE_PATH = DTA_FOLDER_PATH / VALID_FILE_NAME\n","\n","DATASET_BINARY_FILE = Path(\"dataset.pkl\")\n","DATASET_BINARY_PATH = DTA_FOLDER_PATH / DATASET_BINARY_FILE\n","\n","RESULT_FOLDER_PATH = Path(\"./results\")\n","TREE_FOLDER_PATH = Path(\"./trees\")\n","\n","### CODE BLOCK:\n","# Read file\n","def test_read_file():\n","    s = []\n","    with codecs.open(TRAIN_FILE_PATH, 'r', encoding=\"unicode_escape\") as f:\n","        r = f.readline()\n","        s.append(r)\n","        cnt = 1\n","        total = 1\n","        while r:\n","            total += 1\n","            try:\n","                r = f.readline()\n","                s.append(r)\n","                cnt += 1\n","            except:\n","                continue\n","        print(f\"Read {cnt} lines out of {total} lines.\")\n","\n","# Test read line!\n","def test_read_line(s):\n","    head = 1\n","    left = 10\n","    for i in range (head):\n","        row = s[left + i]\n","        print(row)\n","\n","# get_dataset_query\n","def get_dataset_query():\n","    d = get_dataset()\n","    return DatasetQuery(d)\n","    \n","# Print random data\n","# def print_random_data(q: DatasetQuery):\n","#     q.print_random_data()\n","\n","# Test parsing\n","def test_parse_attributes():\n","    import re\n","    import collections\n","\n","    attr = \"(Colors:blue, white,Special Note::very nice,Style: Modern, sdf, sadf  sd, sdf.,,asd,dad)\"\n","    parse_string = attr[1:-1]\n","    colon_separated = re.split(r\"\\s*:+\\s*\", parse_string)\n","    # Actual return data\n","    attr_data = collections.defaultdict(list)\n","    # Current key is guaranteed to be not None\n","    cur_key = None\n","\n","    for i, element in enumerate(colon_separated):\n","        # Separated by comma\n","        comma_separated = re.split(r\"\\s*,\\s*\", element)\n","        # If it is not the last item, and parse according to the pattern\n","        if i != len(colon_separated) - 1:\n","            for j, element_attr in enumerate(comma_separated):\n","                # If it is the last element\n","                if j == len(comma_separated) - 1:\n","                    cur_key = element_attr.lower()\n","                else:\n","                    attr_data[cur_key].append(element_attr.lower())\n","        # Else, all element in the comma separated \n","        #   list is the attribute of the last key\n","        else:\n","            for j, element_attr in enumerate(comma_separated):\n","                attr_data[cur_key].append(element_attr.lower())\n","\n","    print(str(attr_data))\n","\n","def get_dataset() -> Dataset:\n","    p = Parser(TRAIN_FILE_PATH)\n","    dataset = p.parse_data_file()\n","    return dataset\n","\n","# Get image and write file\n","def test_get_image_and_write_file(uri: str, path: str):\n","    import requests\n","    from PIL import Image\n","    f = open(path,'wb')\n","    f.write(requests.get(uri).content)\n","    f.close()\n","    f = Image.open(path)\n","    # data = np.asarray(f)\n","\n","# Get image and save to variabel for numpy array\n","def get_image_and_save_to_variable(uri: str) -> np.array:\n","    import io\n","    import requests\n","    from PIL import Image\n","    import numpy as np\n","    res = requests.get(uri)\n","    image_bytes = io.BytesIO(res.content)\n","    img = Image.open(image_bytes)\n","    img_mat = np.asarray(img)\n","    return img_mat\n","\n","# Test image work\n","# q = get_dataset_query()\n","# dta = q.get_random_data()\n","# uri = dta.primary_image_url\n","# img_dta = dta.get_image(dta.primary_image_url)\n","# np_img_dta = np.asarray(img_dta)\n","# print(\"Shape: \" + str(np_img_dta.shape))\n","\n","def print_random_data(q: DatasetQuery, category: int):\n","    # Get randome data and image\n","    dta = q.get_random_data(category)\n","    print(dta)\n","    return dta.get_image(dta.primary_image_url)\n","\n","def print_data(d: Dataset, data_id: int) -> None:\n","    dta = d.get_data(data_id)\n","    print(dta)\n","    plt.figure()\n","    plt.imshow(dta.get_image(dta.primary_image_url))\n","    plt.show()\n","\n","def print_list_data(d: Dataset, l: List[int]) -> None:\n","    for data_id in l:\n","        print_data(d, data_id)\n","\n","def print_similarity(model: Word2Vec, d: Dataset, key:str, id1: int, id2: int) -> float:\n","    v1: np.ndarray = Preprocessor.encode_sentence(model, d, id1, key)\n","    v2: np.ndarray = Preprocessor.encode_sentence(model, d, id2, key)\n","    return float(cosine_similarity(v1.reshape(1,-1), v2.reshape(1,-1)))\n","\n","def matplot_plot(dataset: Dataset):\n","    # Print random data in each category\n","    import matplotlib.pyplot as plt\n","    import random\n","    rows = 4\n","    cols = 4\n","    for k, v in dataset.category_map.items():\n","        fig = plt.figure()\n","        axes=[]\n","        print(f\"Category {k}: \")\n","        for a in range(rows*cols):\n","            id = random.randint(0, len(v) - 1)\n","            b = dataset.dataset[v[id]].get_image(dataset.dataset[v[id]].primary_image_url)\n","            axes.append( fig.add_subplot(rows, cols, a+1) )\n","            # subplot_title=(\"Subplot\"+str(a))\n","            # axes[-1].set_title(subplot_title)\n","            plt.imshow(b)\n","        # fig.tight_layout()\n","        plt.show()\n","\n","def get_attr_str(dta: Data):\n","    dta.attributes\n","    t = [k + \" \" + (\" \".join(v)) for k, v in dta.attributes.items()]\n","    t = \" \".join(t)\n","    return t\n","\n","def print_first_frequent_keys(d, category: int, head: int=15):\n","    pre = Preprocessor(d)\n","    w = pre.get_key_weights(category)\n","    odic = sorted(w.items(), key=lambda kv:(kv[1], kv[0]), reverse=True)\n","    value_iter = iter(odic)\n","    for _ in range(head):\n","        print(next(value_iter))\n","\n","def write_all_data(q: DatasetQuery):\n","\n","    # Write data analysis every unique attrs\n","    p = \"./analysis/\"\n","    for i in range (1, 6):\n","        dd = q.get_all_unique_key_attributes(i)\n","        \n","        name = p + \"Unique_attrs_cat\" + str(i) + \".txt\"\n","        DatasetQuery.export_txt(dd, name)\n","\n","    p = \"./analysis/\"\n","    for i in range (1, 6):\n","        dd = q.get_all_unique_key_attributes(i)\n","        name = p + \"unique_attr_cnt\" + str(i) + \".csv\"\n","        # DatasetQuery.export_txt(dd, name)\n","        DatasetQuery.export_csv(dd.items(), name)\n","\n","def write_all_unique_key_value_cnt(q: DatasetQuery):\n","    analysis_path = \"./analysis/\"\n","    #  Export csv of every value\n","    for i in range (1, 6):\n","        cnt = q.get_all_unique_key_value_attrs(i)\n","        # Convert from cnt to 2d list\n","        # cnt: Dict[Counter] \n","        #   or Dict[Dict[int]]: attrs_key, attr_value(key), count\n","        rows = []\n","        for key, attr_obs in cnt.items():\n","            for attr, count in attr_obs.items():\n","                rows.append([key, attr, count])\n","        name = analysis_path + \"unique_attr_key_val_cnt\" + str(i) + \".csv\"\n","        DatasetQuery.export_csv(rows, name)\n","\n","\n","def test_vectorize_all_database(d: Dataset, category: int):\n","    models: Dict[str, Word2Vec] = collections.defaultdict()\n","    sentences_matrix: Dict[str, List[List[str]]]= collections.defaultdict(list)\n","\n","    print(\"Extracting value data sentence to glob...\")\n","    with tqdm(total=len(d.category_map[str(category)])) as pbar:\n","        for data_id in d.category_map[str(category)]:\n","            data = d.dataset[data_id]\n","            for key_attr, val_attrs in data.attributes.items():\n","                sentences_matrix[key_attr].append(val_attrs)\n","            pbar.update(1)\n","    print(\"Done!\")\n","\n","    print(\"Creating Word2Vec models from globs...\")\n","    with tqdm(total=len(sentences_matrix.items())) as pbar:\n","        for key, sentences in sentences_matrix.items():\n","            models[key] = Word2Vec(sentences, min_count=1)\n","            pbar.update(1)\n","    print(\"Done!\")\n","\n","\n",""]},{"cell_type":"code","execution_count":2,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Loading dataset...\n","Done!\n"]}],"source":["\n","# Clustering the data according to category 1 ----- Main Code -----\n","\n","print(\"Loading dataset...\")\n","d = Dataset.load_dataset_from_binary(DATASET_BINARY_PATH)\n","q = DatasetQuery(d)\n","print(\"Done!\")\n",""]},{"cell_type":"code","execution_count":3,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Extracting every key to key_list and build model vector...\n"," 22%|██▏       | 72835/327870 [00:00<00:00, 360775.90it/s]Extracting value data sentence to glob...\n","100%|██████████| 327870/327870 [00:00<00:00, 363651.82it/s]\n","  0%|          | 0/8 [00:00<?, ?it/s]\n","Done!\n","Creating Word2Vec models from globs...\n","100%|██████████| 8/8 [00:30<00:00,  3.77s/it]\n","Done!\n","\n"]}],"source":["\n","print(\"Extracting every key to key_list and build model vector...\")\n","n_first_key_to_cluster = 9\n","key_list = q.get_most_frequent_keys(2, n_first_key_to_cluster)[:, 0]\n","# I dont want euro size\n","key_list = [*key_list[0:7], *key_list[8:]]\n","\n","# models.keys() is\n","# dict_keys(['brand', 'inseam', 'size type', \"bottoms size women's\", 'material'])\n","models: Dict[str, Word2Vec] = Embedding.extract_keys_vocab(d, 2, key_list)\n",""]},{"cell_type":"code","execution_count":4,"metadata":{},"outputs":[{"output_type":"stream","name":"stdout","text":["Building tree...\n","Clusterizing according to key \"color\"...\n","  2%|▏         | 6029/327870 [09:08<14:15:03,  6.27it/s]"]}],"source":["\n","print(\"Building tree...\")\n","# Working from here\n","head = -1\n","birch_tree = BirchTree(d, 2, models, head=head)\n","tree = birch_tree.build_tree(verbose=False)\n","\n","# birch_tree.save_birch_tree_to_binary(TREE_FOLDER_PATH)\n","\n","post_processor = PostProcessor(birch_tree, starting_id=500000)\n","post_processor.process()\n","\n","post_processor.export_tsv(RESULT_FOLDER_PATH, 2, n_first_key_to_cluster, head)\n",""]}],"nbformat":4,"nbformat_minor":2,"metadata":{"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":3},"orig_nbformat":2}}